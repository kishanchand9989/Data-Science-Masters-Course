{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffea8f9-a63c-4735-a0a3-074b8aa3a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kishan Chand                          Assignment                        Mar15-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ef3ca-0fe6-41e4-8827-414b01d17422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994434d-bc35-4d74-bff5-42e515381241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e7aa14-22a7-4bd8-9bb6-a06bede9349a",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1909c-90c8-47e1-ba07-26cbb74148bc",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data for certain observations or variables. It occurs when no data or a placeholder value (e.g., NaN, null) is recorded for a particular attribute in a given instance.\n",
    "\n",
    "Handling missing values is essential for several reasons:\n",
    "\n",
    "1. Biased Analysis: Missing values can introduce bias in the analysis and modeling process. If the missingness is not random and is related to the target variable or other variables, it can lead to inaccurate conclusions and biased predictions.\n",
    "\n",
    "2. Incomplete Insights: Missing values can result in incomplete insights and incomplete understanding of the data. Ignoring missing values can lead to a loss of valuable information and hinder the ability to draw meaningful conclusions from the dataset.\n",
    "\n",
    "3. Model Performance: Many machine learning algorithms cannot handle missing values and require complete data for training and prediction. Leaving missing values untreated can result in errors or incorrect model outputs.\n",
    "\n",
    "4. Data Integrity: Missing values can affect the integrity of the dataset, making it difficult to perform statistical analyses, visualize data, or conduct further data processing tasks.\n",
    "\n",
    "Some algorithms that are not affected by missing values or can handle them effectively include:\n",
    "\n",
    "1. Tree-Based Algorithms: Decision trees, Random Forests, and Gradient Boosting methods like XGBoost and LightGBM can handle missing values by effectively splitting the data based on available features.\n",
    "\n",
    "2. K-Nearest Neighbors (KNN): KNN imputation can be used to fill missing values by finding the nearest neighbors of the missing data points and using their values to impute the missing values.\n",
    "\n",
    "3. Gaussian Mixture Models (GMM): GMM can handle missing values by estimating the distribution of the observed data and imputing missing values based on the estimated distribution.\n",
    "\n",
    "4. Deep Learning Models: Deep learning models, such as neural networks, can handle missing values by learning the underlying patterns and relationships in the data, allowing them to make predictions even with missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42411d-6a1f-4589-8f8d-1d2638d81207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb7c2206-b4b1-4516-acf6-f1e47a97f5f5",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f48bdc-5275-4909-b760-30dae432be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean Imputation\n",
    "Median Imputation \n",
    "Mode Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719f57b-314d-4307-9e0f-5ec5aee79d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5923ed0b-9d3f-4547-96b6-4ed2adb84e58",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1566ec9-46fe-4bcc-a3b7-c86e5f0a5958",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a dataset where the distribution of target classes is heavily skewed, \n",
    "meaning that one class is significantly more prevalent than the others. \n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several issues:\n",
    "\n",
    "1. Biased Model: When a dataset is imbalanced, machine learning models tend to be biased towards the majority class. The model learns to predict the majority class more accurately, neglecting the minority class. As a result, the model's performance in correctly predicting the minority class is typically poor.\n",
    "\n",
    "2. Poor Generalization: Imbalanced data can lead to poor generalization of the model to new, unseen data. Since the model is biased towards the majority class, it may struggle to make accurate predictions on samples from the minority class in real-world scenarios.\n",
    "\n",
    "3. Misleading Evaluation Metrics: Traditional evaluation metrics such as accuracy can be misleading when dealing with imbalanced data. If the majority class dominates the dataset, a model that simply predicts the majority class for all samples can achieve high accuracy. However, this does not reflect the model's ability to correctly classify the minority class, which is often the class of interest in many applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71f68c-5b94-43f8-8b53-713ffab7e141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08741962-4695-4a7d-930f-7d70c570eac4",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328a5b2-4efc-4ef0-862f-00d1e1fac858",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to address class imbalance in a dataset by adjusting the class distribution.\n",
    "\n",
    "1. Up-sampling:\n",
    "Up-sampling involves increasing the number of samples in the minority class to match the number of samples in the majority class. This technique helps to balance the class distribution by generating synthetic or duplicate samples from the minority class. It is typically used when the minority class has limited representation in the dataset.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset for fraud detection, where only 5% of the samples are labeled as fraudulent transactions. The class imbalance poses a challenge for training a model that accurately detects fraud. In this case, up-sampling can be applied to generate synthetic samples of fraudulent transactions, increasing their representation in the dataset. This helps the model to learn and identify the patterns and features associated with fraudulent activities more effectively.\n",
    "\n",
    "2. Down-sampling:\n",
    "Down-sampling involves reducing the number of samples in the majority class to match the number of samples in the minority class. This technique helps to balance the class distribution by randomly removing samples from the majority class. It is typically used when the majority class has a significantly higher number of samples compared to the minority class.\n",
    "\n",
    "Example:\n",
    "Consider a dataset for customer churn prediction, where only 10% of the customers have churned. The class imbalance can lead to a biased model that performs poorly in predicting churn. In this scenario, down-sampling can be applied to randomly remove non-churned customer samples from the majority class until the class distribution is balanced. This ensures that the model is trained on an equal representation of both churned and non-churned customers, improving its ability to accurately predict customer churn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf49c3-8e8f-4696-b0be-9c55582ad60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70e7d857-b4ef-4cf6-8392-e8ccdd5ab858",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d105d6-2aad-48cf-82d8-c747d78f1fe0",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in machine learning and data analysis to artificially increase the size of a dataset by creating new synthetic samples. It involves applying various transformations or modifications to the existing data to generate additional samples that are representative of the original data distribution. Data augmentation is particularly useful when the original dataset is limited, and more diverse training examples are needed to improve the model's performance and generalization.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation method used to address the class imbalance problem in binary classification tasks. Class imbalance occurs when one class has significantly fewer samples than the other, leading to biased model training and poor predictive performance on the minority class\n",
    "\n",
    "SMOTE works by generating synthetic samples for the minority class by interpolating between the feature vectors of neighboring samples. Here's how SMOTE works:\n",
    "\n",
    "Select a minority class sample from the dataset.\n",
    "Find its k nearest neighbors in the feature space.\n",
    "Randomly select one of the nearest neighbors.\n",
    "Create a new synthetic sample by interpolating between the selected sample and the chosen neighbor.\n",
    "Repeat the process to generate a desired number of synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f1fc3-9fbc-4d93-a93a-d99c9a11cd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df0410df-fb25-477c-bf41-905d04baebdf",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75588bbb-6756-482c-8453-a9c36abf4fc8",
   "metadata": {},
   "source": [
    "Outliers in a dataset refer to data points that significantly deviate from the majority of other data points. These are observations that are either unusually high or low compared to the rest of the data. \n",
    "\n",
    "Handling outliers is essential for several reasons:\n",
    "\n",
    "Impact on Statistical Measures: Outliers can heavily influence statistical measures such as the mean and standard deviation. These measures are sensitive to extreme values, and outliers can distort the overall understanding of the dataset's central tendency and variability.\n",
    "\n",
    "Skewed Analysis: Outliers can bias the results of data analysis and statistical modeling. They can lead to incorrect conclusions, skewed distributions, and unreliable predictions. It is important to identify and handle outliers to ensure accurate and valid analysis.\n",
    "\n",
    "Model Performance: Outliers can have a significant impact on the performance of machine learning models. Models may give excessive weight to outliers, leading to poor generalization and predictive capabilities. Handling outliers can help improve the model's performance and robustness.\n",
    "\n",
    "Data Integrity: Outliers may indicate data quality issues, such as measurement errors or data collection problems. It is crucial to identify and investigate outliers to ensure data integrity and reliability. By addressing outliers, you can improve the overall quality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2fce4-3461-45e4-aa23-4e993ff3f93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c5fe7c4-4559-4622-9709-3577d5c0a553",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff48884-4444-49dd-abcd-13c5d234c8a0",
   "metadata": {},
   "source": [
    "Below are some commonly used techniques to address the missingness:\n",
    "\n",
    "1. Deletion: By removing the rows or columns with missing data. If the amount of missing data is small and randomly distributed, deleting the rows with missing values may not significantly impact the analysis. However, if the missing data is substantial or non-random, deleting rows or columns can result in loss of valuable information and potential bias.\n",
    "\n",
    "2. Imputation: Imputation is the process of filling in missing values with estimated or imputed values.\n",
    "\n",
    "   a. Mean/Mode/Median Imputation: Replace missing values with the mean, mode, or median of the available data for that variable. This method assumes that the missing values are missing completely at random (MCAR) and does not account for any underlying patterns or relationships.\n",
    "\n",
    "\n",
    "3. We can also recommended to consult domain experts and consider the potential impact of missing data on the validity and reliability of  results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da2c05-e015-40f4-8dea-b9efb017e5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85afaa3a-0423-4e11-8a2d-201e1bb55117",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbce04-ade2-446d-92f4-64240b61a2af",
   "metadata": {},
   "source": [
    "When dealing with missing data in a large dataset, it is important to determine whether the missing data is missing at random (MAR) or if there is a pattern or mechanism behind the missingness. Here are some strategies  can help  to assess the missing data patterns:\n",
    "\n",
    "1. Missing Data Visualization: Begin by visualizing the missing data patterns in the dataset. Plotting missing data patterns, such as using heatmaps or bar charts, can help identify if there are any visible patterns or correlations in the missingness. Look for specific variables or groups of variables where missing values tend to occur more frequently.\n",
    "\n",
    "2. Missing Data Mechanism Tests: Conduct statistical tests to assess the mechanism behind the missing data. Some common tests include the Little's MCAR (Missing Completely at Random) test, the Missing Indicator test, and the Missingness Pattern test. These tests help determine if the missingness is random or if there is a systematic pattern.\n",
    "\n",
    "3. Multiple Imputation: Utilize multiple imputation techniques to impute missing values based on observed data. By imputing the missing values and comparing the imputed data with the observed data, you can evaluate the plausibility of different missing data mechanisms. If the imputed values align well with the observed data, it suggests that the missing data may be MAR.\n",
    "\n",
    "4. Domain Knowledge and Expertise: Tap into domain knowledge and subject matter expertise to gain insights into potential reasons behind the missing data. Collaborate with domain experts who are familiar with the data generation process and can provide valuable insights into the patterns and mechanisms of missingness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acd953-20fd-4300-abd6-9aca395428c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "058b9e56-68ff-4f08-9059-62a963aeb1ad",
   "metadata": {},
   "source": [
    "## 9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35eea89-8b87-4468-af35-9ed594ed8084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab2215f-d64b-4251-af4a-e264365e74f4",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset where the majority class is dominant, So to evaluate the performance of  machine learning model, there are several strategies we can use. Here are some commonly employed techniques:\n",
    "    \n",
    "\n",
    "Confusion Matrix: The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives. It allows to assess the model's performance in classifying both the majority and minority classes accurately.\n",
    "\n",
    "Precision, Recall, and F1 Score: Precision represents the proportion of true positive predictions out of all positive predictions, recall represents the proportion of true positive predictions out of all actual positive instances, and the F1 score is the harmonic mean of precision and recall. These metrics are useful for evaluating model performance on imbalanced datasets\n",
    "\n",
    "Area Under the Receiver Operating Characteristic Curve (AUC-ROC): The AUC-ROC metric is widely used to assess the performance of a binary classification model on imbalanced datasets. It measures the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) across various probability thresholds. A higher AUC-ROC value indicates better model performance.\n",
    "\n",
    "Precision-Recall Curve: The precision-recall curve visualizes the trade-off between precision and recall for different probability thresholds. It is particularly useful when the focus is on correctly identifying the minority class and is informative for selecting an appropriate threshold based on the desired precision and recall trade-off.\n",
    "\n",
    "Stratified Sampling and Cross-Validation: To ensure fair evaluation, it is essential to use appropriate sampling techniques during model evaluation. Stratified sampling and cross-validation methods can help maintain the class distribution ratios during training and evaluation, ensuring that performance metrics are representative of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505e186-017e-48b1-9f14-435deffd08aa",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ece2e-1b23-42ed-bd86-4b084e58daa9",
   "metadata": {},
   "source": [
    "Basically,Customer satisfaction can vary b/w lot of factors or features they like on respective product so to downsample and balance the majority class to minority class we can use below following methods to downsample the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a86b0-eb25-482f-b87a-bb94ba259c1d",
   "metadata": {},
   "source": [
    "Cluster-Based Undersampling: Similar to cluster-based oversampling, this method involves clustering instances from the majority class and then undersampling each cluster to create a more balanced dataset. It helps to preserve the underlying distribution of the majority class.\n",
    "\n",
    "Edited Nearest Neighbors: Edited Nearest Neighbors (ENN) is an undersampling technique that removes instances from the majority class if their class label differs from the majority class label of their k nearest neighbors. This method can be effective in removing noisy instances from the majority class.\n",
    "\n",
    "Tomek Links: Tomek Links are pairs of instances from different classes that are close to each other but belong to different classes. Undersampling based on Tomek Links involves removing the majority class instances from these pairs, which can help in creating a more balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae166e-0127-4d74-86e2-114103b2caea",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71670f-f123-4687-bcb1-fdec1c3a0753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e53e85f-a1e1-480b-82ec-1006b3ce95be",
   "metadata": {},
   "source": [
    "We can use the following methods to up sample the minority class to balance the dataset\n",
    "\n",
    "Synthetic Minority Oversampling Technique (SMOTE),which creates synthetic examples for the minority class by interpolating between neighboring instances. It selects an instance from the minority class and finds its k nearest neighbors. It then creates synthetic instances along the line segments connecting the instance to its neighbors.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN),this is an extension of SMOTE that generates synthetic instances for the minority class based on the density of the instances. It pays more attention to the instances that are harder to learn by generating more synthetic instances near them.\n",
    "\n",
    "Cluster-Based Oversampling: This method involves clustering instances from the minority class and then oversampling each cluster to create a more balanced dataset. It helps to preserve the underlying distribution of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd1a00-79dc-4c73-a3df-95389f65532a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c314f1c-560e-495b-83bb-c9ee0fafd0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
