{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79a2c9d-5c81-4436-b4bf-a1bd143166da",
   "metadata": {},
   "source": [
    "        Kishan Chand                         Assignment                        Mar19-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff68fa-ad2a-4ea0-88d5-2a302087826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f1c9d-73cf-4bd1-90c6-5979706d2d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de24bfba-e765-4bc3-a317-2516d304d8a6",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7432c61-cfec-4f10-b349-cf0d455a733a",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform the values of numerical features within a specific range. It scales the values proportionally, mapping the minimum value to one end of the range (usually 0) and the maximum value to the other end (usually 1). This ensures that all values lie within the defined range.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Consider a dataset with a numerical feature representing house prices. Let's assume we have the following data points:\n",
    "\n",
    "Data Point 1: 250,000\n",
    "Data Point 2: 300,000\n",
    "Data Point 3: 200,000\n",
    "\n",
    "Step 1: Determine the minimum and maximum values\n",
    "Find the minimum and maximum values of the feature within the dataset.\n",
    "\n",
    "Minimum Value: 200,000\n",
    "Maximum Value: 300,000\n",
    "\n",
    "Step 2: Apply Min-Max scaling\n",
    "Scale each data point using the following formula:\n",
    "\n",
    "Scaled Value = (Original Value - Minimum Value) / (Maximum Value - Minimum Value)\n",
    "\n",
    "Scaled Data Point 1 = (250,000 - 200,000) / (300,000 - 200,000) = 0.5\n",
    "Scaled Data Point 2 = (300,000 - 200,000) / (300,000 - 200,000) = 1.0\n",
    "Scaled Data Point 3 = (200,000 - 200,000) / (300,000 - 200,000) = 0.0\n",
    "\n",
    "The resulting scaled values now range from 0 to 1, with 0 representing the minimum value and 1 representing the maximum value.\n",
    "\n",
    "Min-Max scaling is commonly used in data preprocessing to ensure that features with different scales are on a similar scale, facilitating fair comparison and preventing certain features from dominating the analysis or modeling process.\n",
    "\n",
    "It's important to note that Min-Max scaling is sensitive to outliers since it stretches the range based on the minimum and maximum values. Outliers can distort the scaling and affect the distribution of the data. Therefore, it is advisable to handle outliers before applying Min-Max scaling or consider alternative scaling techniques like Standardization (Z-score scaling) in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4abf0-81d9-4318-8084-21e8fc21c0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d3ef9-b90c-4654-afcd-f084deb6cf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e1d17a2-32a7-453e-9e0f-0387923b0361",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b4f8c-35eb-4fef-935c-56ad311ac6cb",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Vector Normalization, is a feature scaling method that scales the values of each feature in a dataset to have a unit norm, i.e., a magnitude of 1. It differs from Min-Max scaling as it focuses on the direction or orientation of the data rather than the range.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Consider a dataset with two numerical features: height (in centimeters) and weight (in kilograms). Let's assume we have the following data points:\n",
    "\n",
    "Data Point 1: [175, 70]\n",
    "Data Point 2: [150, 55]\n",
    "Data Point 3: [180, 80]\n",
    "\n",
    "Step 1: Compute the magnitude of each data point\n",
    "Calculate the magnitude of each data point using the Euclidean norm. The Euclidean norm of a vector [x, y] is calculated as sqrt(x^2 + y^2).\n",
    "\n",
    "Magnitude of Data Point 1 = sqrt(175^2 + 70^2) = 186.34\n",
    "Magnitude of Data Point 2 = sqrt(150^2 + 55^2) = 159.13\n",
    "Magnitude of Data Point 3 = sqrt(180^2 + 80^2) = 193.23\n",
    "\n",
    "Step 2: Normalize the data points\n",
    "Divide each data point by its corresponding magnitude to obtain the unit vector representation.\n",
    "\n",
    "Normalized Data Point 1 = [175/186.34, 70/186.34] = [0.939, 0.376]\n",
    "Normalized Data Point 2 = [150/159.13, 55/159.13] = [0.942, 0.336]\n",
    "Normalized Data Point 3 = [180/193.23, 80/193.23] = [0.931, 0.414]\n",
    "\n",
    "The resulting data points are now unit vectors, meaning they have a magnitude of 1. This scaling technique ensures that the direction or orientation of the data is preserved while eliminating the influence of the original magnitude.\n",
    "\n",
    "Compared to Min-Max scaling, which scales the data within a specified range, Unit Vector scaling is particularly useful when the magnitude of the features is not relevant, and the focus is primarily on the direction or relative relationships between the features.\n",
    "\n",
    "It's important to note that the Unit Vector technique is sensitive to outliers since it normalizes the entire vector. Outliers with extreme values can have a disproportionate impact on the resulting unit vectors. Therefore, it is advisable to handle outliers before applying the Unit Vector technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3033eee-ea74-4221-a962-2d0cb8f72452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082442fe-be57-4823-8b25-872b882406de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c472948c-0f3e-495b-9754-08c4c6960d2a",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434c6c0-2da3-473d-a3b5-72e080769e4f",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while preserving the most important information in the data. It achieves this by identifying the directions (principal components) along which the data varies the most.\n",
    "\n",
    "Here's an example to illustrate the application of PCA in dimensionality reduction:\n",
    "\n",
    "Let's say we have a dataset with three features: height, weight, and age. Each data point represents an individual. We want to reduce the dimensionality of the dataset from three to two while retaining as much relevant information as possible.\n",
    "\n",
    "Step 1: Normalize the data\n",
    "Normalize the data by subtracting the mean and scaling each feature to have unit variance. This step ensures that all features have the same scale and prevents features with larger magnitudes from dominating the PCA.\n",
    "\n",
    "Step 2: Perform PCA\n",
    "Perform PCA on the normalized dataset. PCA will calculate the principal components, which are the directions along which the data varies the most. In this case, since we have three features, PCA will identify three principal components.\n",
    "\n",
    "Step 3: Select the desired number of components\n",
    "Choose the number of principal components to retain based on the desired level of dimensionality reduction. In this example, we want to reduce the dimensionality from three to two, so we select the first two principal components.\n",
    "\n",
    "Step 4: Transform the data\n",
    "Transform the original dataset using the selected principal components. The transformed dataset will have a reduced dimensionality, with each sample represented by the values along the retained principal components.\n",
    "\n",
    "The resulting reduced-dimensional dataset can be visualized in a 2D scatter plot, where each data point is represented by its projection onto the first two principal components. This plot allows us to visualize the data in a lower-dimensional space while preserving the most important patterns and relationships among the samples.\n",
    "\n",
    "By using PCA for dimensionality reduction, we have effectively compressed the information in the original dataset into a lower-dimensional representation. This can be useful for various purposes, such as data visualization, computational efficiency in machine learning algorithms, and dealing with the curse of dimensionality.\n",
    "\n",
    "It's important to note that PCA assumes linearity in the data and may not be suitable for datasets with nonlinear relationships. In such cases, other nonlinear dimensionality reduction techniques like t-SNE or LLE may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26526203-efad-43c0-8c87-0aaaf88b4d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aab807-be42-4af0-8ea6-de7a80284302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27df305d-f391-4d6e-b499-05853ae1cac3",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccaecc1-e793-4284-b6c5-970169187a0e",
   "metadata": {},
   "source": [
    "\n",
    "Here's the relationship between PCA and feature extraction:\n",
    "\n",
    "1. **Dimensionality Reduction:** Both PCA and feature extraction aim to reduce the number of dimensions (features) in the data while preserving the most important information. This can help to overcome the \"curse of dimensionality\" and improve the efficiency and interpretability of machine learning algorithms.\n",
    "\n",
    "2. **Variance Maximization:** PCA identifies the directions (principal components) along which the data has the maximum variance. These directions correspond to the most informative features, and projecting the data onto these components helps to capture the most variation in the data.\n",
    "\n",
    "3. **Decorrelation:** One of the goals of PCA is to decorrelate the features, which means that the principal components are orthogonal to each other. This reduces multicollinearity and helps in capturing independent and meaningful patterns in the data.\n",
    "\n",
    "Here's an example of how PCA can be used for feature extraction:\n",
    "\n",
    "Let's say we have a dataset of images with high-resolution pixel values as features. Each image is 100x100 pixels,\n",
    "resulting in 10,000-dimensional data points.The high dimensionality might lead to computational inefficiency and noise in the data. \n",
    "so we can use PCA for feature extraction to represent each image using a smaller number of principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb255d0-9903-4c4d-ab01-056f4eac205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (1000, 10000)\n",
      "Transformed data shape: (1000, 500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate synthetic image data (replace with your actual data)\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "n_features = 10000\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "n_components = 500  # Number of principal components to keep\n",
    "pca = PCA(n_components=n_components)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "# Now X_transformed contains the images' representations using the selected principal components\n",
    "print(\"Original data shape:\", X.shape)\n",
    "print(\"Transformed data shape:\", X_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68c400-c3c0-432c-aab9-57aeb86d9e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483fe48e-c0b7-40ed-8027-4415ddb717fc",
   "metadata": {},
   "source": [
    "\n",
    "In this example, you reduce the dimensionality of the image data from 10,000 features to 5000 principal components. \n",
    "These 50 principal components represent the most important features of the images and can be used for downstream tasks such as clustering, classification, or visualization.\n",
    "\n",
    "In summary, PCA is a powerful technique that not only reduces dimensionality but also helps in extracting relevant and meaningful features from high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c4882-5e2d-4ade-a421-56f849740417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc078a80-01ea-4aa1-8783-b9b0678c5310",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f040d2-f0c2-4962-80c9-1876815a2ea6",
   "metadata": {},
   "source": [
    "When building a recommendation system for a food delivery service, it's important to preprocess the data to ensure that all features are on a similar scale. Min-Max scaling is one approach to achieve this.\n",
    "\n",
    "Min-Max scaling, also known as normalization, rescales the features to a specific range, typically between 0 and 1. This is done by subtracting the minimum value of the feature and dividing it by the range (maximum value minus minimum value).\n",
    "\n",
    "Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. Identify the feature(s) that you want to scale. In this case, it could be price, rating, and delivery time.\n",
    "\n",
    "2. Calculate the minimum and maximum values for each feature in the dataset.\n",
    "\n",
    "3. For each value in the feature, subtract the minimum value and divide it by the range (maximum value minus minimum value). This will rescale the values to a range between 0 and 1.\n",
    "\n",
    "4. After applying Min-Max scaling, the feature values will be transformed to the desired range, making them comparable and preventing any single feature from dominating the recommendation process.\n",
    "\n",
    "By using Min-Max scaling, you ensure that each feature is on a similar scale, which helps in comparing and combining different features during the recommendation process. It also prevents features with larger numerical values from having a disproportionate impact on the recommendation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef1e74-258c-44c7-bd2c-430a6450e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a83667-0856-402f-aa69-e66c09732246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6744b5ba-a668-48ac-8cd9-7c4123ffd6c4",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85057cb2-cdfc-4235-b2ab-f4259dd9f287",
   "metadata": {},
   "source": [
    "When working with a dataset that contains numerous features for predicting stock prices, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Here's an explanation of how PCA can be applied in this scenario:\n",
    "\n",
    "1. Data Preparation: Ensure the dataset is properly preprocessed, including handling missing values, normalization, and standardization if necessary.\n",
    "\n",
    "2. Feature Selection: Before applying PCA, it's recommended to perform feature selection techniques to identify the most relevant features for predicting stock prices. This step helps to reduce noise and improve the efficiency of PCA.\n",
    "\n",
    "3. PCA Application: Once the relevant features are selected, PCA can be applied to further reduce the dimensionality. PCA works by transforming the original features into a new set of uncorrelated variables called principal components. These components are linear combinations of the original features.\n",
    "\n",
    "4. Variance Explained: Determine the number of principal components to retain based on the amount of variance they explain. Each principal component captures a certain amount of variance in the dataset. By selecting a subset of principal components that capture a significant portion of the variance (e.g., 90% or higher), you can reduce the dimensionality while still retaining most of the information.\n",
    "\n",
    "5. Dimensionality Reduction: Use the selected principal components to represent the dataset with reduced dimensionality. These components can serve as new features for training your stock price prediction model.\n",
    "\n",
    "6. Model Training: Finally, you can train your stock price prediction model using the transformed dataset with reduced dimensionality. The reduced feature space can potentially improve model performance by reducing overfitting, computational complexity, and noise in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c5806-dd1c-4f7c-a3cb-dd4ba6a805ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe262d7-75f4-483b-bff5-f963e0c55a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142bedbe-b920-4a01-97d0-64e06c4e6b43",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f6772f-8522-46ba-a205-893bba985d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =[1,5,10,15,20]\n",
    "min_value =min(df)\n",
    "print(min_value)\n",
    "max_value =max(df)\n",
    "print(max_value)\n",
    "transformed_values =[]\n",
    "for i in df:\n",
    "    transformed_values.append(((i - min_value) / (max_value - min_value))) #* (max_range - min_range) + min_range))\n",
    "transformed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27fc394-cf0d-44d6-a3a1-9183a952ddfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe52296-d793-4ac9-a9f3-2d79ddb165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset = [-1, -0.5, 0, 0.5, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d2fae-3078-4bee-bb9a-5f51949f2cb9",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6b6ae-4779-4a94-9e50-d62436a47edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ac8e03b-0863-43fe-a1a5-77efe3c3ff59",
   "metadata": {},
   "source": [
    "Basically, retaining of features mainly  depends on the specific dataset,and  its characteristics, and the goals of the analysis. Experimenting with different numbers of components and evaluating their impact on the performance of subsequent tasks (e.g., classification or regression) can help determine the optimal number of components for the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8a10-378a-4cb0-892c-1eaaee61db4e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For the given dataset with features [height, weight, age, gender, blood pressure], PCA would be applied as follows:\n",
    "\n",
    "1. Standardize the Data: Before applying PCA, it is important to standardize the features to have zero mean and unit variance. This ensures that each feature contributes equally to the analysis.\n",
    "\n",
    "2. Compute the Covariance Matrix: The covariance matrix is computed from the standardized data. The covariance measures the relationship between each pair of features and indicates how they vary together.\n",
    "\n",
    "3. Calculate the Eigenvectors and Eigenvalues: The next step is to calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions or components in the original feature space, while the eigenvalues indicate the importance of these components.\n",
    "\n",
    "4. Sort the Eigenvalues: Sort the eigenvalues in descending order. The eigenvalues represent the amount of variance explained by each principal component. The higher the eigenvalue, the more important the corresponding component.\n",
    "\n",
    "5. Select the Principal Components: Determine the number of principal components to retain based on the explained variance criteria discussed earlier. This can be done by examining the cumulative explained variance ratio or using the elbow method.\n",
    "\n",
    "6. Project the Data onto the Principal Components: Finally, the original dataset is transformed into the new coordinate system defined by the selected principal components. This projection involves multiplying the standardized data by the selected eigenvectors corresponding to the retained principal components.\n",
    "\n",
    "The resulting transformed dataset will have the same number of samples but with reduced dimensions. The retained principal components will capture the most significant variations in the data, while the discarded components represent the less important variations. This dimensionality reduction can help in visualizing and analyzing the data, as well as potentially improving the performance of subsequent machine learning tasks by reducing the noise and redundancy in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16b26d-c3dd-404a-bea3-da096d1545bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59a677-88a8-4445-af56-7cb8ad92e4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973bc477-baa7-4a52-b6e8-c8066d9b136f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc3cb2-c4b2-406d-be93-9b44e1820a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
