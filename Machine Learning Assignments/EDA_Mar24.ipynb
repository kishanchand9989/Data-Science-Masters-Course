{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea80f46e-a4c7-4171-9f54-4d3c75daf333",
   "metadata": {},
   "source": [
    "      Kishan Chand                            Assignment                          Mar24-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a45d9-561d-4a6f-91f1-1059b9cbc829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069aaf70-da1c-4446-8459-88301aa51db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1145d91b-73f4-46ad-af6e-0b8f45060ff6",
   "metadata": {},
   "source": [
    "# Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in predicting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f35a5f-a4cd-43b9-9f8f-2249a89fe817",
   "metadata": {},
   "source": [
    "With the help of Heat plot (correlation map),I can say that there were only four key feature attributes were showing positive correlation with target column \"quality\" \n",
    "\n",
    "they are :\n",
    "\n",
    "alcohols,sulphates,critic acid,fixed acidity\n",
    "\n",
    "\n",
    "\n",
    "## 1.alcohols\n",
    "-> alcohols showing high correlation with target column(quality) of 0.48, which shows the significance of alcohol quantity in wine\n",
    "\n",
    "## 2.sulphates\n",
    "-> sulphates showing the positive correlation of 0.25 w.r.t target column(quality)\n",
    "\n",
    "-> the distribution of sulphates follows right skewwed distribution.\n",
    "## 3.citric acid\n",
    "-> citric acid  showing the positive correlation of 0.23 w.r.t target column(quality)\n",
    "\n",
    "-> the distribution of citric acid follows unsymmetric distribution.\n",
    "\n",
    "## 4. fixed acidity\n",
    "-> fixed acidity showing the positive correlation of 0.12 w.r.t target column(quality)\n",
    "\n",
    "-> which is follwed by the Normal Distribution graph with an mean of 8.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef31ac3-6ace-4e69-bbfa-582155071043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ad1d5b2-cf05-4e43-b62b-de14d86b0fed",
   "metadata": {},
   "source": [
    "\n",
    "# Q2. How did you handle missing data in the wine quality data set during the feature engineering process? Discuss the advantages and disadvantages of different imputation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10363a9a-b731-488b-9c4b-29ceed1302b3",
   "metadata": {},
   "source": [
    "## Basically, there was no Missing data existed inside the wine quality dataset,Incase if missing data existed inside the data we can handle it using imputation techniques such as mean median mode imputations,Although it is continuos numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b74f990-00da-45fe-a5dd-d75894256e35",
   "metadata": {},
   "source": [
    "Imputation is the process of filling in missing or incomplete data values in a dataset. Different imputation techniques have their own advantages and disadvantages, and the choice of technique depends on the nature of the data and the problem at hand.\n",
    "Here are some common imputation techniques along with their pros and cons:\n",
    "\n",
    "1. **Mean/Median Imputation:**\n",
    "   - **Advantages:** Simple and quick to implement, doesn't distort the distribution of the variable much, works well when data is missing at random.\n",
    "   - **Disadvantages:** Ignores any correlations between variables, can lead to biased estimates if data is not missing at random.\n",
    "\n",
    "2. **Mode Imputation:**\n",
    "   - **Advantages:** Suitable for categorical variables, preserves the original distribution of the variable.\n",
    "   - **Disadvantages:** Ignores relationships between variables, might not be suitable for continuous variables.\n",
    "\n",
    "3. **Regression Imputation:**\n",
    "   - **Advantages:** Takes into account relationships between variables, can provide more accurate estimates.\n",
    "   - **Disadvantages:** Sensitive to outliers and multicollinearity, requires a well-fitted regression model.\n",
    "\n",
    "4. **K-Nearest Neighbors (KNN) Imputation:**\n",
    "   - **Advantages:** Uses information from other data points, suitable for both continuous and categorical variables.\n",
    "   - **Disadvantages:** Computationally intensive, choice of K (number of neighbors) can impact results.\n",
    "\n",
    "5. **Multiple Imputation:**\n",
    "   - **Advantages:** Takes into account uncertainty in imputations, provides more accurate standard errors and confidence intervals.\n",
    "   - **Disadvantages:** Can be computationally expensive, requires assumptions about the missing data mechanism.\n",
    "\n",
    "6. **Hot Deck Imputation:**\n",
    "   - **Advantages:** Preserves the relationships between variables, works well when data is missing at random.\n",
    "   - **Disadvantages:** Can lead to data leakage, not suitable for large datasets.\n",
    "\n",
    "7. **Interpolation Techniques (Time Series):**\n",
    "   - **Advantages:** Suitable for time series data, considers the temporal order of observations.\n",
    "   - **Disadvantages:** Might not work well for irregularly spaced time series, sensitive to outliers.\n",
    "\n",
    "8. **Deep Learning Imputation:**\n",
    "   - **Advantages:** Can capture complex relationships in the data, suitable for large datasets.\n",
    "   - **Disadvantages:** Requires significant computational resources, might overfit if not properly tuned.\n",
    "\n",
    "9. **Domain-Specific Imputation:**\n",
    "   - **Advantages:** Uses domain knowledge to impute missing values, can lead to more accurate results.\n",
    "   - **Disadvantages:** Requires expertise in the specific domain, might not work well if domain knowledge is limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2943e7-4a3c-4e72-9795-a77433c10492",
   "metadata": {},
   "source": [
    "# Q3. What are the key factors that affect students' performance in exams? How would you go about analyzing these factors using statistical techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42800ad-23c8-4d75-b4ab-b932ef5ecce8",
   "metadata": {},
   "source": [
    "1. lunch,test_preparation_course,race_ethnicity these three key factors that affect the student's performance during the exams,\n",
    "   As these columns are categorical,I'll try to plot a histogram with kde representation to observe/know the distribution of data  by applying hue    parmeter to distinguish the categories. \n",
    "   \n",
    " 2. i'll try to calculate the mean and median of students total score w.r.t each sub-categorical column of lunch,test_preparation,race_ethnicity to analyse the students total score performance\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c0107-33d5-4073-987f-e343718f1745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07c9b7d7-e349-4a15-922b-76b549abec97",
   "metadata": {},
   "source": [
    "# Q4. Describe the process of feature engineering in the context of the student performance data set. How did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07604e5c-35d3-4786-b61d-74cec4119df0",
   "metadata": {},
   "source": [
    "\n",
    "As part of Feature Engineering i have calculated two columns known as total_score, average_score,which is calculated with the help of reading_score,writing_score,math_score which helps to examine the performance of student's performance.\n",
    "\n",
    "when comes to selection,Student performances are Evaluated/judged by  using  their Total scores and Average scores which determines student performance. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2d1cf-9be7-48b4-a830-6debb94996c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2091c9b-4fc3-4157-b015-086b2a51dc21",
   "metadata": {},
   "source": [
    "# Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc13c36-f393-49b5-80d6-97986ab4a249",
   "metadata": {},
   "source": [
    "These are the features that exhibit non-normality \n",
    "\n",
    "### 1.alcohol -  unsymmetric distribution \n",
    "### 2.citric acid -unsymmetric distribution\n",
    "### 3.sulphates -Right Skewwed distribution\n",
    "### 4.volatile acidity - unsymmetric distribution\n",
    "### 5.chlorides - Right Skewed Distribution\n",
    "### 6.residual sugar -Right skewed distribution\n",
    "### 7.total sulpurdioxide - Right skewwed distribution\n",
    "\n",
    "\n",
    "\n",
    "Several transformations can be applied to features to improve normality in their distributions:\n",
    "\n",
    "Logarithmic Transformation: Useful for reducing right skewness. Applying a logarithmic function (such as log, log10, log1p) to the data can compress larger values, making the distribution more symmetrical.\n",
    "\n",
    "Square Root Transformation: Effective for reducing right skewness and handling data with smaller values. It can be applied to moderate skewness to make the distribution more symmetrical.\n",
    "\n",
    "Box-Cox Transformation: A power transformation that automatically identifies the best power transformation to achieve normality. It can handle various types of skewness and asymmetry.\n",
    "\n",
    "Sigmoid Transformation: Converts data into a sigmoidal (S-shaped) distribution. It can help bring extreme values towards the center, making the distribution more symmetric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca30d4-3dd9-46dd-bf07-fa54fc8af11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7398aa33-d358-4a1b-9777-5e68543b370b",
   "metadata": {},
   "source": [
    "# Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of features. What is the minimum number of principal components required to explain 90% of the variance in the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990aef04-71df-4ee8-b137-db82a54d4166",
   "metadata": {},
   "source": [
    "# Note:- We can use Wine quality dataset and Student Performance Data set as per the discussion in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eadf41f-c61f-4619-8881-8dbe8648d968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2c643f-eb5a-429c-aec2-00fb2347d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.28173931 0.1750827  0.1409585  0.11029387 0.08720837 0.05996439\n",
      " 0.05307193 0.03845061 0.0313311  0.01648483 0.00541439]\n",
      "\n",
      "Cumulative Explained Variance Ratio: [0.28173931 0.45682201 0.59778051 0.70807438 0.79528275 0.85524714\n",
      " 0.90831906 0.94676967 0.97810077 0.99458561 1.        ]\n",
      "\n",
      "Number of Principal Components to explain 90% variance: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load and preprocess the wine quality dataset\n",
    "wine_data = pd.read_csv(\"winequality-red.csv\")\n",
    "X = wine_data.drop(\"quality\", axis=1)  # Assuming \"quality\" is the target variable\n",
    "y = wine_data[\"quality\"]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the minimum number of principal components to explain 90% of variance\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.9) + 1\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print()\n",
    "print(\"Cumulative Explained Variance Ratio:\", cumulative_variance_ratio)\n",
    "print()\n",
    "print(\"Number of Principal Components to explain 90% variance:\", num_components)\n",
    "\n",
    "# explained_variance_ratio array contains the proportion of variance explained by each principal component. \n",
    "# The cumulative_variance_ratio array contains the cumulative proportion of variance explained by the principal components.\n",
    "# num_components variable holds the minimum number of principal components required to explain 90% of the variance in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf7a6d-4b9f-43b1-b52c-4b10f0f34ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad336c-04c7-40b5-8fd2-3ec4937be775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
